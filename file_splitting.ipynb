{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from astropy.time import Time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed453a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelim frequency reduction, remove files with frequencies all less than 2 GHz\n",
    "\n",
    "\n",
    "directory = \"/datag/users/ctremblay/\"\n",
    "\n",
    "# Get list of candidate files\n",
    "files = [f for f in os.listdir(directory) if f.startswith(\"Summer_Project\") and f.endswith(\".pkl\")]\n",
    "full_paths = [os.path.join(directory, f) for f in files]\n",
    "\n",
    "def check_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "        if not df.empty and 'signal_frequency' in df.columns:\n",
    "            min_freq = df['signal_frequency'].min()\n",
    "            if pd.notna(min_freq) and min_freq >= 2000:\n",
    "                return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(check_file, full_paths)\n",
    "\n",
    "    high_freq_files = [r for r in results if r is not None]\n",
    "    print(f\"\\n {len(high_freq_files)} files with frequency ≥ 2000 MHz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Load Crickets CSV ---\n",
    "crickets_path = \"Full_Crickets.csv\"\n",
    "intervals = pd.read_csv(crickets_path)\n",
    "intervals.columns = intervals.columns.str.strip()\n",
    "\n",
    "# Rename columns for clarity\n",
    "intervals = intervals.rename(columns={\n",
    "    \"rfi_bin_bots\": \"start_frequency\",\n",
    "    \"rfi_bin_tops\": \"end_frequency\"\n",
    "})\n",
    "\n",
    "# Ensure frequencies are floats and drop invalid rows\n",
    "intervals['start_frequency'] = pd.to_numeric(intervals['start_frequency'], errors='coerce')\n",
    "intervals['end_frequency'] = pd.to_numeric(intervals['end_frequency'], errors='coerce')\n",
    "intervals = intervals.dropna(subset=['start_frequency', 'end_frequency'])\n",
    "\n",
    "# --- 2. Define NRAO manual bands ---\n",
    "# --- 2. Define NRAO manual bands ---\n",
    "nrao_ranges = [\n",
    "    (2178.0, 2195.0), (2106.4, 2106.4), (2204.5, 2204.5), (2180, 2290),\n",
    "    (2227.5, 2231.5), (2246.5, 2252.5), (2268.5, 2274.5), (2282.5, 2288.5),\n",
    "    (2314.5, 2320.5), (2320.0, 2332.5), (2324.5, 2330.5), (2332.5, 2345.0),\n",
    "    (2334.5, 2340.5), (2387.5, 2387.5), (2400.0, 2483.5), (2411.0, 2413.0),\n",
    "    (2483.5, 2500.0), (2741.0, 2741.0), (2791.0, 2791.0), (3700.0, 4200.0),\n",
    "    (5648.5, 5663.5), (5659.5, 5670.5), (5695.5, 5704.5), (5742.5, 5757.5),\n",
    "    (5765.0, 5769.0), (5796.0, 5804.0), (6108.1, 6138.1), (6137.75, 6167.75),\n",
    "    (6182.0, 6212.0), (6360.14, 6390.14), (6389.79, 6419.79), (6772.0, 6778.0),\n",
    "    (7250.0, 7850.0), (9300.0, 9900.0), (9300.0, 9500.0), (10740.0, 10770.0),\n",
    "    (10820.0, 10850.0), (10957.0, 10993.0), (11037.0, 11073.0), (11230.0, 11260.0),\n",
    "    (11310.0, 11340.0), (11447.0, 11483.0), (11527.0, 11563.0), (11700.0, 12000.0),\n",
    "    (12000.0, 12700.0), (13400.0, 13750.0), (17800.0, 20200.0), (29500.0, 30000.0),\n",
    "    (34875.0, 34875.0), (36286.0, 36286.0)\n",
    "]\n",
    "\n",
    "# --- 3. Combine Crickets + Manual RFI bands ---\n",
    "manual_df = pd.DataFrame(nrao_ranges, columns=['start_frequency', 'end_frequency'])\n",
    "combined_df = pd.concat([intervals[['start_frequency', 'end_frequency']], manual_df], ignore_index=True)\n",
    "\n",
    "# --- 4. Merge overlapping bands ---\n",
    "def merge_overlapping_bands(bands, tol=1e-6):\n",
    "    bands = sorted(bands, key=lambda x: x[0])\n",
    "    merged = []\n",
    "    for band in bands:\n",
    "        if not merged:\n",
    "            merged.append(band)\n",
    "        else:\n",
    "            prev_start, prev_end = merged[-1]\n",
    "            curr_start, curr_end = band\n",
    "            if curr_start <= prev_end + tol:\n",
    "                # Overlapping or adjacent\n",
    "                merged[-1] = (prev_start, max(prev_end, curr_end))\n",
    "            else:\n",
    "                merged.append(band)\n",
    "    return merged\n",
    "\n",
    "# Convert to list of tuples, merge\n",
    "combined_bands = list(zip(combined_df['start_frequency'], combined_df['end_frequency']))\n",
    "merged_bands = merge_overlapping_bands(combined_bands)\n",
    "\n",
    "# --- 5. Save merged bands to CSV ---\n",
    "merged_df = pd.DataFrame(merged_bands, columns=['start_frequency', 'end_frequency'])\n",
    "save_path = os.path.join(os.getcwd(), \"Full_Crickets_merged.csv\")\n",
    "merged_df.to_csv(save_path, index=False)\n",
    "print(f\" Saved merged RFI band list to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NRAO and CRICKETS RFI frequency elimination\n",
    "\n",
    "# --- Update these paths ---\n",
    "filtered_files = high_freq_files  # List of full file paths\n",
    "output_dir = '/datax/scratch/ellambishop/rfi_removed_hits'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Load and clean RFI bands ---\n",
    "intervals = pd.read_csv(\"Full_Crickets_merged.csv\")\n",
    "intervals.columns = intervals.columns.str.strip()\n",
    "intervals = intervals.rename(columns={\n",
    "    \"rfi_bin_bots\": \"start_frequency\",\n",
    "    \"rfi_bin_tops\": \"end_frequency\"\n",
    "})\n",
    "intervals['start_frequency'] = pd.to_numeric(intervals['start_frequency'], errors='coerce')\n",
    "intervals['end_frequency'] = pd.to_numeric(intervals['end_frequency'], errors='coerce')\n",
    "rfi_bands = intervals.dropna(subset=['start_frequency', 'end_frequency'])[['start_frequency', 'end_frequency']].values\n",
    "\n",
    "\n",
    "def filter_hits_by_rfi(df):\n",
    "    \"\"\"Vectorized RFI filtering using NumPy broadcasting.\"\"\"\n",
    "    freqs = df['signal_frequency'].values\n",
    "    keep_mask = np.ones(len(freqs), dtype=bool)\n",
    "\n",
    "    for low, high in rfi_bands:\n",
    "        keep_mask &= ~((freqs >= low) & (freqs <= high))  # Mask out RFI frequencies\n",
    "\n",
    "    return df[keep_mask]\n",
    "\n",
    "\n",
    "def process_file(filepath):\n",
    "    input_path = filepath\n",
    "    output_path = os.path.join(output_dir, os.path.basename(filepath))\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(input_path)\n",
    "\n",
    "        # Pre-filter: only keep signal_frequency ≥ 2000 MHz\n",
    "        df = df[df['signal_frequency'] >= 2000]\n",
    "\n",
    "        # Apply vectorized RFI filtering\n",
    "        df_filtered = filter_hits_by_rfi(df)\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            print(f\"{filepath}: No hits remain after filtering, skipping save.\")\n",
    "            return\n",
    "\n",
    "        df_filtered.to_pickle(output_path)\n",
    "        print(f\"{filepath}: Saved filtered hits ({len(df_filtered)} rows)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Processing {len(filtered_files)} files with multiprocessing...\")\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        pool.map(process_file, filtered_files)\n",
    "\n",
    "    print(\"Done filtering all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f56bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique incoherent frequencies: 1798338\n",
      "Total unique phase_center frequencies: 3059255\n"
     ]
    }
   ],
   "source": [
    "#flag frequencies with incoherent counterpart \n",
    "file_list = glob.glob(\"/datax/scratch/ellambishop/rfi_removed_hits/Summer*.pkl\")\n",
    "\n",
    "global_incoherent_freqs = set()\n",
    "global_phase_center_freqs = set()\n",
    "\n",
    "for file in file_list:\n",
    "    if 'Dec-45' in file:\n",
    "        continue  # Skip large files\n",
    "    try:\n",
    "        df = pd.read_pickle(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "    if \"source_name\" in df.columns and \"signal_frequency\" in df.columns:\n",
    "        source_lower = df[\"source_name\"].fillna(\"\").str.lower()\n",
    "        incoherent_freqs = df.loc[source_lower.str.contains(\"incoherent\"), \"signal_frequency\"].unique()\n",
    "        phase_center_freqs = df.loc[source_lower.str.contains(\"phase_center\"), \"signal_frequency\"].unique()\n",
    "        global_incoherent_freqs.update(incoherent_freqs)\n",
    "        global_phase_center_freqs.update(phase_center_freqs)\n",
    "\n",
    "print(f\"Total unique incoherent frequencies: {len(global_incoherent_freqs)}\")\n",
    "print(f\"Total unique phase_center frequencies: {len(global_phase_center_freqs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9200006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/vlass_incoherent.pkl\n",
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/vlass_phase_center.pkl\n",
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/vlass_other.pkl\n",
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/non_vlass_incoherent.pkl\n",
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/non_vlass_phase_center.pkl\n",
      "Saved: /datax/scratch/ellambishop/temp_hits_organized/non_vlass_other.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize category buckets again\n",
    "vlass_incoherent = []\n",
    "vlass_phase_center = []\n",
    "vlass_other = []\n",
    "\n",
    "non_vlass_incoherent = []\n",
    "non_vlass_phase_center = []\n",
    "non_vlass_other = []\n",
    "\n",
    "for file in file_list:\n",
    "    if 'Dec-45' in file:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"file_uri\" in df.columns and \"source_name\" in df.columns and \"signal_frequency\" in df.columns:\n",
    "        source_lower = df[\"source_name\"].fillna(\"\").str.lower()\n",
    "\n",
    "        # Add frequency counterpart flags using the global sets\n",
    "        df[\"freq_has_incoherent_counterpart\"] = df[\"signal_frequency\"].isin(global_incoherent_freqs)\n",
    "        df[\"freq_has_phase_center_counterpart\"] = df[\"signal_frequency\"].isin(global_phase_center_freqs)\n",
    "\n",
    "        vlass_mask = df[\"file_uri\"].str.contains(\"vlass\", case=False, na=False)\n",
    "        df_vlass = df[vlass_mask].copy()\n",
    "        df_non_vlass = df[~vlass_mask].copy()\n",
    "\n",
    "        def categorize_and_append(df_subset, incoh_list, phase_list, other_list):\n",
    "            source_lower_sub = df_subset[\"source_name\"].fillna(\"\").str.lower()\n",
    "            incoherent_mask = source_lower_sub.str.contains(\"incoherent\")\n",
    "            phase_center_mask = source_lower_sub.str.contains(\"phase_center\")\n",
    "            other_mask = ~(incoherent_mask | phase_center_mask)\n",
    "\n",
    "            if incoherent_mask.any():\n",
    "                incoh_list.append(df_subset[incoherent_mask])\n",
    "            if phase_center_mask.any():\n",
    "                phase_list.append(df_subset[phase_center_mask])\n",
    "            if other_mask.any():\n",
    "                other_list.append(df_subset[other_mask])\n",
    "\n",
    "        if not df_vlass.empty:\n",
    "            categorize_and_append(df_vlass, vlass_incoherent, vlass_phase_center, vlass_other)\n",
    "\n",
    "        if not df_non_vlass.empty:\n",
    "            categorize_and_append(df_non_vlass, non_vlass_incoherent, non_vlass_phase_center, non_vlass_other)\n",
    "\n",
    "output_dir = \"/datax/scratch/ellambishop/temp_hits_organized\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_if_not_empty(dfs_list, filename):\n",
    "    if dfs_list:\n",
    "        out_path = os.path.join(output_dir, filename)\n",
    "        pd.concat(dfs_list, ignore_index=True).to_pickle(out_path)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "save_if_not_empty(vlass_incoherent, \"vlass_incoherent.pkl\")\n",
    "save_if_not_empty(vlass_phase_center, \"vlass_phase_center.pkl\")\n",
    "save_if_not_empty(vlass_other, \"vlass_other.pkl\")\n",
    "\n",
    "save_if_not_empty(non_vlass_incoherent, \"non_vlass_incoherent.pkl\")\n",
    "save_if_not_empty(non_vlass_phase_center, \"non_vlass_phase_center.pkl\")\n",
    "save_if_not_empty(non_vlass_other, \"non_vlass_other.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3adfbec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98718 98718\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('/datax/scratch/ellambishop/temp_hits_organized/vlass_incoherent.pkl')\n",
    "df1 = pd.read_pickle('/datax/scratch/ellambishop/new_hits_organized/vlass_incoherent.pkl')\n",
    "print(len(df), len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see which params have least number of unique objects \n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Track unique values for each column\n",
    "global_uniques = defaultdict(set)\n",
    "\n",
    "directory = '/datax/scratch/ellambishop/rfi_removed_hits/'\n",
    "total = 0\n",
    "\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    if 'Dec-45' in file:\n",
    "        continue  # Skip large files\n",
    "\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    print(f\"Processing {file} ({len(df)} rows)\")\n",
    "    total += len(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Limit to only object/int/str columns (skip arrays, large blobs)\n",
    "        if df[col].dtype.kind in {'O', 'i', 'u', 'S'}:\n",
    "            unique_vals = df[col].unique()\n",
    "            global_uniques[col].update(unique_vals)\n",
    "\n",
    "# Sort and print columns with fewest unique values\n",
    "print(\"\\nGlobal grouping candidates with fewest unique values:\")\n",
    "summary = {col: len(vals) for col, vals in global_uniques.items()}\n",
    "for col, count in sorted(summary.items(), key=lambda x: x[1]):\n",
    "    print(f\"{col:<25} → {count} unique values\")\n",
    "\n",
    "print(f\"\\nTotal rows across all files: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGlobal grouping candidates with fewest unique values:\")\n",
    "summary = {col: len(vals) for col, vals in global_uniques.items()}\n",
    "for col, count in sorted(summary.items(), key=lambda x: x[1]):\n",
    "    print(f\"{col:<25} → {count} unique values\")\n",
    "\n",
    "print(f\"\\nTotal rows across all files: {total}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
